{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl (25.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ann_system(hub1_name, hub2_name, validation_size, test_size, window_size, verbose=True, save=True):\n",
    "    hub1 = pd.read_csv(f\"../../data/interpolated/{hub1_name}_close_interpolated.csv\")\n",
    "    hub2 = pd.read_csv(f\"../../data/interpolated/{hub2_name}_close_interpolated.csv\")\n",
    "\n",
    "    hub1 = hub1.rename(columns={\"CLOSE\": \"hub1_CLOSE\"})\n",
    "    hub2 = hub2.rename(columns={\"CLOSE\": \"hub2_CLOSE\"})\n",
    "    hub1_hub2_diff = pd.DataFrame(hub1[\"hub1_CLOSE\"] - hub2[\"hub2_CLOSE\"], columns=[\"hub1_hub2_diff\"], index=hub1.index)\n",
    "\n",
    "    # Shift columns and store in new columns for hub1, hub2, and hub1_hub2_diff\n",
    "    for i in range(window_size, window_size + 6):\n",
    "        hub1[f\"hub1_CLOSE-{i- window_size}\"] = hub1[\"hub1_CLOSE\"].shift(i)\n",
    "        hub2[f\"hub2_CLOSE-{i - window_size}\"] = hub2[\"hub2_CLOSE\"].shift(i)\n",
    "        hub1_hub2_diff[f\"hub1_hub2_diff-{i - window_size}\"] = hub1_hub2_diff[\"hub1_hub2_diff\"].shift(i)\n",
    "\n",
    "    # Concatenate and drop NaN rows in one step\n",
    "    data = pd.concat([hub1, hub2, hub1_hub2_diff], axis=1).dropna()\n",
    "\n",
    "    features = [\n",
    "        'hub1_CLOSE-0', #'hub1_CLOSE-1', #'hub1_CLOSE-2', 'hub1_CLOSE-3', 'hub1_CLOSE-4', 'hub1_CLOSE-5',\n",
    "        'hub2_CLOSE-0', #'hub2_CLOSE-1', #'hub2_CLOSE-2', 'hub2_CLOSE-3', 'hub2_CLOSE-4', 'hub2_CLOSE-5',\n",
    "        'hub1_hub2_diff-0', 'hub1_hub2_diff-1', 'hub1_hub2_diff-2', 'hub1_hub2_diff-3', 'hub1_hub2_diff-4', 'hub1_hub2_diff-5',# 'hub1_hub2_diff-6'\n",
    "    ]\n",
    "\n",
    "    X = data[features].values\n",
    "\n",
    "    y = data[['hub1_CLOSE', 'hub2_CLOSE']].values\n",
    "\n",
    "    X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "    y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    #X_train, X_val = X_train[:-validation_size], X_train[-validation_size:]\n",
    "    #y_train, y_val = y_train[:-validation_size], y_train[-validation_size:]\n",
    "\n",
    "    # Build a simple ANN model\n",
    "    model = Sequential([\n",
    "        Dense(2, activation='relu'),\n",
    "        #Dense(1, activation='linear'),\n",
    "        Dense(2)  # 2 outputs for both hubs\n",
    "    ])\n",
    "    #lr = 0.0038336791635842195 #TTF-THE\n",
    "    #lr = 0.00026670003536052503 #TTF-NBP\n",
    "    #lr = 0.0018046087209813773 #THE-NBP 2\n",
    "    lr = 0.003319584493448066 #THE-NBP\n",
    "    #lr = 0.001\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mape')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=25, batch_size=1, \n",
    "                        #validation_data=(X_val, y_val), \n",
    "                        shuffle=False,\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape_hub1 = mean_absolute_percentage_error(y_test[:, 0], predictions[:, 0]) * 100\n",
    "    print(f\"MAPE for {hub1_name}: {mape_hub1:.2f}%\")\n",
    "\n",
    "    # Calculate MAPE for Hub 2\n",
    "    mape_hub2 = mean_absolute_percentage_error(y_test[:, 1], predictions[:, 1]) * 100\n",
    "    print(f\"MAPE for {hub2_name}: {mape_hub2:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "    return hub1, hub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1285, 8) (1285, 2) (250, 8) (250, 2)\n",
      "Epoch 1/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 554us/step - loss: 10.0788\n",
      "Epoch 2/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543us/step - loss: 7.7384\n",
      "Epoch 3/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546us/step - loss: 7.6237\n",
      "Epoch 4/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 536us/step - loss: 7.5603\n",
      "Epoch 5/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 548us/step - loss: 7.5572\n",
      "Epoch 6/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551us/step - loss: 7.4848\n",
      "Epoch 7/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543us/step - loss: 7.4455\n",
      "Epoch 8/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 7.5068\n",
      "Epoch 9/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 542us/step - loss: 7.4964\n",
      "Epoch 10/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547us/step - loss: 7.5807\n",
      "Epoch 11/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 556us/step - loss: 7.5522\n",
      "Epoch 12/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 7.5527\n",
      "Epoch 13/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568us/step - loss: 7.6426\n",
      "Epoch 14/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571us/step - loss: 7.7244\n",
      "Epoch 15/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579us/step - loss: 7.7007\n",
      "Epoch 16/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 7.8018\n",
      "Epoch 17/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705us/step - loss: 7.8102\n",
      "Epoch 18/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 7.8222\n",
      "Epoch 19/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 7.8582\n",
      "Epoch 20/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576us/step - loss: 7.8778\n",
      "Epoch 21/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633us/step - loss: 7.8592\n",
      "Epoch 22/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 7.8977\n",
      "Epoch 23/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step - loss: 7.9219\n",
      "Epoch 24/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 8.0038\n",
      "Epoch 25/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 7.9660\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.8245\n",
      "Test Loss: 6.056476593017578\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MAPE for ttf: 6.04%\n",
      "MAPE for nbp: 6.07%\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "hub1 = \"ttf\"\n",
    "hub2 = \"nbp\"\n",
    "validation_size = 250\n",
    "test_size = 250\n",
    "window_size = 5\n",
    "\n",
    "data = ann_system(hub1, hub2, validation_size, test_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:59:24,163] A new study created in memory with name: no-name-694e848d-adde-4223-8158-e184097c3642\n",
      "[I 2024-10-30 16:59:35,434] Trial 0 finished with value: 11.255555152893066 and parameters: {'units_0': 2, 'learning_rate': 0.0016299789780296827, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 16:59:46,732] Trial 1 finished with value: 14.031111717224121 and parameters: {'units_0': 2, 'learning_rate': 0.0030795676314273866, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 16:59:53,570] Trial 2 finished with value: 14.627769470214844 and parameters: {'units_0': 2, 'learning_rate': 0.00563858375827385, 'batch_size': 4}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:00,687] Trial 3 finished with value: 16.94601821899414 and parameters: {'units_0': 2, 'learning_rate': 0.0067717188549123826, 'batch_size': 4}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:12,314] Trial 4 finished with value: 82.8842544555664 and parameters: {'units_0': 2, 'learning_rate': 0.0010092225954398675, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:24,186] Trial 5 finished with value: 11.973058700561523 and parameters: {'units_0': 2, 'learning_rate': 0.0015979803318110115, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:31,478] Trial 6 finished with value: 87.18216705322266 and parameters: {'units_0': 2, 'learning_rate': 0.0012424571005329955, 'batch_size': 4}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:38,504] Trial 7 finished with value: 87.92459106445312 and parameters: {'units_0': 2, 'learning_rate': 0.0011418574291118203, 'batch_size': 4}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:45,490] Trial 8 finished with value: 75.06779479980469 and parameters: {'units_0': 2, 'learning_rate': 0.005226300500688264, 'batch_size': 4}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:00:57,462] Trial 9 finished with value: 16.09343147277832 and parameters: {'units_0': 2, 'learning_rate': 0.005017122546130253, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:01:09,814] Trial 10 finished with value: 11.634819984436035 and parameters: {'units_0': 2, 'learning_rate': 0.0022435989053909097, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:01:22,001] Trial 11 finished with value: 11.332464218139648 and parameters: {'units_0': 2, 'learning_rate': 0.002487095644705583, 'batch_size': 2}. Best is trial 0 with value: 11.255555152893066.\n",
      "[I 2024-10-30 17:01:34,269] Trial 12 finished with value: 10.934887886047363 and parameters: {'units_0': 2, 'learning_rate': 0.0021246305313405733, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:01:46,380] Trial 13 finished with value: 76.69406127929688 and parameters: {'units_0': 2, 'learning_rate': 0.0019824733669487338, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:01:58,724] Trial 14 finished with value: 12.616303443908691 and parameters: {'units_0': 2, 'learning_rate': 0.001606733541066237, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:02:10,864] Trial 15 finished with value: 13.754268646240234 and parameters: {'units_0': 2, 'learning_rate': 0.003525849359377046, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:02:23,793] Trial 16 finished with value: 14.769848823547363 and parameters: {'units_0': 2, 'learning_rate': 0.0033416721158924475, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:02:35,737] Trial 17 finished with value: 74.23233032226562 and parameters: {'units_0': 2, 'learning_rate': 0.009261234967891076, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:02:47,875] Trial 18 finished with value: 12.319358825683594 and parameters: {'units_0': 2, 'learning_rate': 0.001562112057240171, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:00,177] Trial 19 finished with value: 13.295747756958008 and parameters: {'units_0': 2, 'learning_rate': 0.002671668483379378, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:07,323] Trial 20 finished with value: 11.265653610229492 and parameters: {'units_0': 2, 'learning_rate': 0.0019287430720575194, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:14,548] Trial 21 finished with value: 11.674361228942871 and parameters: {'units_0': 2, 'learning_rate': 0.0019398627214416825, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:22,601] Trial 22 finished with value: 11.234319686889648 and parameters: {'units_0': 2, 'learning_rate': 0.0013559588798050587, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:31,162] Trial 23 finished with value: 11.66856575012207 and parameters: {'units_0': 2, 'learning_rate': 0.0013348025016663666, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:39,549] Trial 24 finished with value: 11.283590316772461 and parameters: {'units_0': 2, 'learning_rate': 0.0014149789806695212, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:51,550] Trial 25 finished with value: 82.7745590209961 and parameters: {'units_0': 2, 'learning_rate': 0.0010213886219686168, 'batch_size': 2}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:03:58,910] Trial 26 finished with value: 11.253093719482422 and parameters: {'units_0': 2, 'learning_rate': 0.0017927944091018528, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:04:07,110] Trial 27 finished with value: 13.42440128326416 and parameters: {'units_0': 2, 'learning_rate': 0.004242091824590477, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:04:15,118] Trial 28 finished with value: 80.23318481445312 and parameters: {'units_0': 2, 'learning_rate': 0.0026362534814083818, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n",
      "[I 2024-10-30 17:04:23,529] Trial 29 finished with value: 11.19391918182373 and parameters: {'units_0': 2, 'learning_rate': 0.0018427393317400826, 'batch_size': 4}. Best is trial 12 with value: 10.934887886047363.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value (MAPE): 10.934887886047363\n",
      "Params:\n",
      "    units_0: 2\n",
      "    learning_rate: 0.0021246305313405733\n",
      "    batch_size: 2\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "MAPE for ttf: 6.11%\n",
      "MAPE for nbp: 6.86%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import optuna\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "hub1_name = \"ttf\"\n",
    "hub2_name = \"the\"\n",
    "\n",
    "def build_model(trial):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of hidden layers and units per layer\n",
    "    #n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_layers = 1\n",
    "    for i in range(n_layers):\n",
    "        units = trial.suggest_int(f\"units_{i}\", 2, 2, step=1)\n",
    "        #units = 1\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(2))  # Output layer for both hubs\n",
    "\n",
    "    # Tune the learning rate\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mape')\n",
    "    return model\n",
    "\n",
    "def load_and_prepare_data(hub1_name, hub2_name, window_size, test_size, validation_size):\n",
    "    hub1 = pd.read_csv(f\"../../data/interpolated/{hub1_name}_close_interpolated.csv\")\n",
    "    hub2 = pd.read_csv(f\"../../data/interpolated/{hub2_name}_close_interpolated.csv\")\n",
    "\n",
    "    hub1 = hub1.rename(columns={\"CLOSE\": \"hub1_CLOSE\"})\n",
    "    hub2 = hub2.rename(columns={\"CLOSE\": \"hub2_CLOSE\"})\n",
    "\n",
    "    hub1_hub2_diff = pd.DataFrame(hub1[\"hub1_CLOSE\"] - hub2[\"hub2_CLOSE\"], columns=[\"hub1_hub2_diff\"], index=hub1.index)\n",
    "\n",
    "    for i in range(window_size, window_size + 7):\n",
    "        hub1[f\"hub1_CLOSE-{i-window_size}\"] = hub1[\"hub1_CLOSE\"].shift(i)\n",
    "        hub2[f\"hub2_CLOSE-{i - window_size}\"] = hub2[\"hub2_CLOSE\"].shift(i)\n",
    "        hub1_hub2_diff[f\"hub1_hub2_diff-{i - window_size}\"] = hub1_hub2_diff[\"hub1_hub2_diff\"].shift(i)\n",
    "\n",
    "    # Concatenate and drop NaN rows in one step\n",
    "    data = pd.concat([hub1, hub2, hub1_hub2_diff], axis=1).dropna()\n",
    "\n",
    "\n",
    "    features = [\n",
    "        'hub1_CLOSE-1', #'hub1_CLOSE-2', 'hub1_CLOSE-3', 'hub1_CLOSE-4', 'hub1_CLOSE-5', 'hub1_CLOSE-6',\n",
    "        'hub2_CLOSE-1', #'hub2_CLOSE-2', 'hub2_CLOSE-3', 'hub2_CLOSE-4', 'hub2_CLOSE-5', 'hub2_CLOSE-6',\n",
    "        'hub1_hub2_diff-0', 'hub1_hub2_diff-1', 'hub1_hub2_diff-2', 'hub1_hub2_diff-3', 'hub1_hub2_diff-4', 'hub1_hub2_diff-5', 'hub1_hub2_diff-6'\n",
    "    ]\n",
    "\n",
    "    X = data[features].values\n",
    "    y = data[['hub1_CLOSE', 'hub2_CLOSE']].values\n",
    "\n",
    "    X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "    y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "    X_train, X_val = X_train[:-validation_size], X_train[-validation_size:]\n",
    "    y_train, y_val = y_train[:-validation_size], y_train[-validation_size:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def objective(trial):\n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_prepare_data(\n",
    "        hub1_name=hub1_name, hub2_name=hub2_name, \n",
    "        window_size=5, test_size=250, validation_size=250\n",
    "    )\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(trial)\n",
    "\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 2,4, step=2)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=25, batch_size=batch_size, \n",
    "              validation_data=(X_val, y_val), verbose=0, shuffle=False)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Display best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value (MAPE): {trial.value}\")\n",
    "print(\"Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = build_model(study.best_trial)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_prepare_data(\n",
    "    hub1_name=hub1_name, hub2_name=hub2_name, window_size=5, test_size=250, validation_size=250\n",
    ")\n",
    "keras.utils.set_random_seed(42)\n",
    "best_batch_size = study.best_trial.params[\"batch_size\"]\n",
    "best_model.fit(np.vstack([X_train, X_val]), np.vstack([y_train, y_val]), shuffle = False, epochs=25, batch_size=best_batch_size, verbose=0)\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Calculate and print MAPE for each hub\n",
    "mape_hub1 = mean_absolute_percentage_error(y_test[:, 0], predictions[:, 0]) * 100\n",
    "mape_hub2 = mean_absolute_percentage_error(y_test[:, 1], predictions[:, 1]) * 100\n",
    "print(f\"MAPE for {hub1_name}: {mape_hub1:.2f}%\")\n",
    "print(f\"MAPE for {hub2_name}: {mape_hub2:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27.218     , 28.39325622, -1.17525622, -1.36686785, -1.25789822,\n",
       "        -1.31696325, -1.18920841, -1.4341256 ],\n",
       "       [26.835     , 28.04845395, -1.21345395, -1.17525622, -1.36686785,\n",
       "        -1.25789822, -1.31696325, -1.18920841],\n",
       "       [26.22      , 27.39920497, -1.17920497, -1.21345395, -1.17525622,\n",
       "        -1.36686785, -1.25789822, -1.31696325],\n",
       "       [25.759     , 26.9094428 , -1.1504428 , -1.17920497, -1.21345395,\n",
       "        -1.17525622, -1.36686785, -1.25789822],\n",
       "       [26.628     , 28.02107415, -1.39307415, -1.1504428 , -1.17920497,\n",
       "        -1.21345395, -1.17525622, -1.36686785],\n",
       "       [26.625     , 27.99749411, -1.37249411, -1.39307415, -1.1504428 ,\n",
       "        -1.17920497, -1.21345395, -1.17525622],\n",
       "       [26.328     , 27.43588221, -1.10788221, -1.37249411, -1.39307415,\n",
       "        -1.1504428 , -1.17920497, -1.21345395],\n",
       "       [25.942     , 27.11229399, -1.17029399, -1.10788221, -1.37249411,\n",
       "        -1.39307415, -1.1504428 , -1.17920497],\n",
       "       [26.025     , 27.0848398 , -1.0598398 , -1.17029399, -1.10788221,\n",
       "        -1.37249411, -1.39307415, -1.1504428 ],\n",
       "       [26.858     , 28.10842734, -1.25042734, -1.0598398 , -1.17029399,\n",
       "        -1.10788221, -1.37249411, -1.39307415]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
