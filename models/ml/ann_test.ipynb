{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:03:23.089106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sindrehaugland/Documents/Master/statistical-arbitrage/.conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ann_system(hub1_name, hub2_name, validation_size, test_size, window_size, mode=\"test\", lr = 0.003):\n",
    "\n",
    "    hub1 = pd.read_csv(f\"../../data/interpolated/{hub1_name}_close_interpolated.csv\")\n",
    "    hub2 = pd.read_csv(f\"../../data/interpolated/{hub2_name}_close_interpolated.csv\")\n",
    "\n",
    "    hub1 = hub1.rename(columns={\"CLOSE\": \"hub1_CLOSE\"})\n",
    "    hub2 = hub2.rename(columns={\"CLOSE\": \"hub2_CLOSE\"})\n",
    "    hub1_hub2_diff = pd.DataFrame(hub1[\"hub1_CLOSE\"] - hub2[\"hub2_CLOSE\"], columns=[\"hub1_hub2_diff\"], index=hub1.index)\n",
    "\n",
    "    # Shift columns and store in new columns for hub1, hub2, and hub1_hub2_diff\n",
    "    for i in range(window_size, window_size + 6):\n",
    "        hub1[f\"hub1_CLOSE-{i- window_size}\"] = hub1[\"hub1_CLOSE\"].shift(i)\n",
    "        hub2[f\"hub2_CLOSE-{i - window_size}\"] = hub2[\"hub2_CLOSE\"].shift(i)\n",
    "        hub1_hub2_diff[f\"hub1_hub2_diff-{i - window_size}\"] = hub1_hub2_diff[\"hub1_hub2_diff\"].shift(i)\n",
    "\n",
    "    # Concatenate and drop NaN rows in one step\n",
    "    data = pd.concat([hub1, hub2, hub1_hub2_diff], axis=1).dropna()\n",
    "\n",
    "    features = [\n",
    "        'hub1_CLOSE-0', #'hub1_CLOSE-1', #'hub1_CLOSE-2', 'hub1_CLOSE-3', 'hub1_CLOSE-4', 'hub1_CLOSE-5',\n",
    "        'hub2_CLOSE-0', #'hub2_CLOSE-1', #'hub2_CLOSE-2', 'hub2_CLOSE-3', 'hub2_CLOSE-4', 'hub2_CLOSE-5',\n",
    "        'hub1_hub2_diff-0', 'hub1_hub2_diff-1', 'hub1_hub2_diff-2', 'hub1_hub2_diff-3', 'hub1_hub2_diff-4', 'hub1_hub2_diff-5',# 'hub1_hub2_diff-6'\n",
    "    ]\n",
    "\n",
    "    X = data[features].values\n",
    "    y = data[['hub1_CLOSE', 'hub2_CLOSE']].values\n",
    "\n",
    "    if mode == \"validation\":\n",
    "        X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "        y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "\n",
    "        X_train, X_val = X_train[:-validation_size], X_train[-validation_size:]\n",
    "        y_train, y_val = y_train[:-validation_size], y_train[-validation_size:]\n",
    "\n",
    "        print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "        y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "\n",
    "        print(X_train.shape, X_test.shape)\n",
    "\n",
    "    \n",
    "    keras.utils.set_random_seed(42)\n",
    "    # Build a simple ANN model\n",
    "    model = Sequential([\n",
    "        Dense(2, activation='relu'),\n",
    "        #Dense(1, activation='linear'),\n",
    "        Dense(2)  # 2 outputs for both hubs\n",
    "    ])\n",
    "    #lr = 0.0033595696621040246 TTF-THE\n",
    "    # Compile the model\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mape')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=25, batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "\n",
    "    if mode == \"validation\":\n",
    "        test_loss = model.evaluate(X_val, y_val, verbose=1)\n",
    "        print(f\"Val Loss: {test_loss}\")\n",
    "\n",
    "        # Predict on test data\n",
    "        predictions = model.predict(X_val)\n",
    "\n",
    "        # Calculate MAPE\n",
    "        mape_hub1 = mean_absolute_percentage_error(y_val[:, 0], predictions[:, 0]) * 100\n",
    "        print(f\"MAPE for {hub1_name}: {mape_hub1:.2f}%\")\n",
    "\n",
    "        # Calculate MAPE for Hub 2\n",
    "        mape_hub2 = mean_absolute_percentage_error(y_val[:, 1], predictions[:, 1]) * 100\n",
    "        print(f\"MAPE for {hub2_name}: {mape_hub2:.2f}%\")\n",
    "    else:\n",
    "        test_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Predict on test data\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Calculate MAPE\n",
    "        mape_hub1 = mean_absolute_percentage_error(y_test[:, 0], predictions[:, 0]) * 100\n",
    "        print(f\"MAPE for {hub1_name}: {mape_hub1:.2f}%\")\n",
    "\n",
    "        # Calculate MAPE for Hub 2\n",
    "        mape_hub2 = mean_absolute_percentage_error(y_test[:, 1], predictions[:, 1]) * 100\n",
    "        print(f\"MAPE for {hub2_name}: {mape_hub2:.2f}%\")\n",
    "\n",
    "    return hub1, hub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1035, 8) (250, 8) (250, 8)\n",
      "Epoch 1/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 79.0164\n",
      "Epoch 2/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 11.4459\n",
      "Epoch 3/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.6550\n",
      "Epoch 4/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.4779\n",
      "Epoch 5/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.5567\n",
      "Epoch 6/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.5174\n",
      "Epoch 7/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.4786\n",
      "Epoch 8/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.5785\n",
      "Epoch 9/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.6180\n",
      "Epoch 10/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.5550\n",
      "Epoch 11/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.5943\n",
      "Epoch 12/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.5906\n",
      "Epoch 13/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.6911\n",
      "Epoch 14/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.6292\n",
      "Epoch 15/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.5986\n",
      "Epoch 16/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.6808\n",
      "Epoch 17/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.7452\n",
      "Epoch 18/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.6999\n",
      "Epoch 19/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.7666\n",
      "Epoch 20/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.7068\n",
      "Epoch 21/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.7841\n",
      "Epoch 22/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.7699\n",
      "Epoch 23/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 8.7405\n",
      "Epoch 24/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.7024\n",
      "Epoch 25/25\n",
      "\u001b[1m1035/1035\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 8.7061\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6119\n",
      "Val Loss: 13.006519317626953\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "MAPE for ttf: 11.44%\n",
      "MAPE for nbp: 14.57%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hub1_name = \"ttf\"\n",
    "hub2_name = \"nbp\"\n",
    "validation_size = 250\n",
    "test_size = 250\n",
    "window_size = 5\n",
    "\n",
    "#=================old=====================\n",
    "#lr = 0.0033595696621040246 # TTF-THE\n",
    "#lr = 0.0020391667368093716 # TTF-NBP\n",
    "#lr = 0.0022680764797231650 # THE-NBP\n",
    "\n",
    "#=================new====================\n",
    "#lr = 0.0034989953431570587 # TTF-THE\n",
    "lr = 0.002011660009751183 # TTF-NBP\n",
    "#lr = 0.0020055959168877725 # THE-NBP\n",
    "\n",
    "hub1, hub2 = ann_system(hub1_name, hub2_name, validation_size, test_size, window_size, mode = \"validation\", lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1285, 8) (250, 8)\n",
      "Epoch 1/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 73.6534\n",
      "Epoch 2/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9325\n",
      "Epoch 3/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 7.8927\n",
      "Epoch 4/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.8417\n",
      "Epoch 5/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8762\n",
      "Epoch 6/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8336\n",
      "Epoch 7/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8098\n",
      "Epoch 8/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.7948\n",
      "Epoch 9/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8003\n",
      "Epoch 10/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8495\n",
      "Epoch 11/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8630\n",
      "Epoch 12/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8674\n",
      "Epoch 13/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8869\n",
      "Epoch 14/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8830\n",
      "Epoch 15/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9345\n",
      "Epoch 16/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9298\n",
      "Epoch 17/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9006\n",
      "Epoch 18/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9578\n",
      "Epoch 19/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9100\n",
      "Epoch 20/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.9416\n",
      "Epoch 21/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.9018\n",
      "Epoch 22/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9484\n",
      "Epoch 23/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.9335\n",
      "Epoch 24/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.9223\n",
      "Epoch 25/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.8930\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.89285\n",
      "Test Loss: 6.149715900421143\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "MAPE for ttf: 6.09%\n",
      "MAPE for nbp: 6.21%\n"
     ]
    }
   ],
   "source": [
    "hub1, hub2 = ann_system(hub1_name, hub2_name, validation_size, test_size, window_size, mode = \"test\", lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 23:10:19,636] A new study created in memory with name: no-name-4f540931-3a5a-460e-a82d-32bf5a70346e\n",
      "[I 2024-11-10 23:10:35,079] Trial 0 finished with value: 10.740342140197754 and parameters: {'lr': 0.002296770340205521}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:10:52,748] Trial 1 finished with value: 10.971541404724121 and parameters: {'lr': 0.002602940817227042}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:11:08,696] Trial 2 finished with value: 11.138235092163086 and parameters: {'lr': 0.002422606379378012}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:11:24,836] Trial 3 finished with value: 10.743795394897461 and parameters: {'lr': 0.0023390239589072925}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:11:41,021] Trial 4 finished with value: 11.064925193786621 and parameters: {'lr': 0.002545308157076079}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:11:57,186] Trial 5 finished with value: 11.091042518615723 and parameters: {'lr': 0.002464207185930536}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:12:14,241] Trial 6 finished with value: 11.13486385345459 and parameters: {'lr': 0.0029855168034129157}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:12:30,922] Trial 7 finished with value: 10.785148620605469 and parameters: {'lr': 0.0020884970717573023}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:12:47,244] Trial 8 finished with value: 10.951312065124512 and parameters: {'lr': 0.002599049047258796}. Best is trial 0 with value: 10.740342140197754.\n",
      "[I 2024-11-10 23:13:03,130] Trial 9 finished with value: 10.631669044494629 and parameters: {'lr': 0.002028757659015767}. Best is trial 9 with value: 10.631669044494629.\n",
      "[I 2024-11-10 23:13:19,398] Trial 10 finished with value: 12.704733848571777 and parameters: {'lr': 0.0034809960925155058}. Best is trial 9 with value: 10.631669044494629.\n",
      "[I 2024-11-10 23:13:36,503] Trial 11 finished with value: 10.561464309692383 and parameters: {'lr': 0.0020869367374690565}. Best is trial 11 with value: 10.561464309692383.\n",
      "[I 2024-11-10 23:13:53,581] Trial 12 finished with value: 10.55078125 and parameters: {'lr': 0.0020055777060710906}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:14:10,780] Trial 13 finished with value: 11.294923782348633 and parameters: {'lr': 0.0029146094582000012}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:14:27,086] Trial 14 finished with value: 10.56003475189209 and parameters: {'lr': 0.0021842011486431626}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:14:42,695] Trial 15 finished with value: 10.804675102233887 and parameters: {'lr': 0.0022318474431939944}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:14:58,540] Trial 16 finished with value: 11.135859489440918 and parameters: {'lr': 0.0027810582865888155}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:15:14,318] Trial 17 finished with value: 11.537555694580078 and parameters: {'lr': 0.003268062737764054}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:15:32,382] Trial 18 finished with value: 10.631558418273926 and parameters: {'lr': 0.002202480289495177}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:15:48,592] Trial 19 finished with value: 11.230086326599121 and parameters: {'lr': 0.0021684675085315366}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:16:04,394] Trial 20 finished with value: 10.633487701416016 and parameters: {'lr': 0.002010837023160462}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:16:20,384] Trial 21 finished with value: 10.774688720703125 and parameters: {'lr': 0.0021377038515693904}. Best is trial 12 with value: 10.55078125.\n",
      "[I 2024-11-10 23:16:36,604] Trial 22 finished with value: 10.550384521484375 and parameters: {'lr': 0.0020055959168877725}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:16:52,825] Trial 23 finished with value: 10.7485990524292 and parameters: {'lr': 0.0023184804170041104}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:17:10,637] Trial 24 finished with value: 10.735958099365234 and parameters: {'lr': 0.0020004359808268018}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:17:27,321] Trial 25 finished with value: 10.738482475280762 and parameters: {'lr': 0.0022162157598868736}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:17:43,348] Trial 26 finished with value: 11.17366886138916 and parameters: {'lr': 0.0024206623784589956}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:17:59,720] Trial 27 finished with value: 10.92140007019043 and parameters: {'lr': 0.002750385413541579}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:18:16,102] Trial 28 finished with value: 10.884666442871094 and parameters: {'lr': 0.002131875444365117}. Best is trial 22 with value: 10.550384521484375.\n",
      "[I 2024-11-10 23:18:33,076] Trial 29 finished with value: 10.781893730163574 and parameters: {'lr': 0.0022884773661000003}. Best is trial 22 with value: 10.550384521484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.0020055959168877725\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "\n",
    "# Modify the ann_system function to accept the learning rate as an argument\n",
    "def optuna_ann_system(hub1_name, hub2_name, validation_size, test_size, window_size, lr,mode=\"validation\", verbose=True, save=True):\n",
    "    hub1 = pd.read_csv(f\"../../data/interpolated/{hub1_name}_close_interpolated.csv\")\n",
    "    hub2 = pd.read_csv(f\"../../data/interpolated/{hub2_name}_close_interpolated.csv\")\n",
    "\n",
    "    hub1 = hub1.rename(columns={\"CLOSE\": \"hub1_CLOSE\"})\n",
    "    hub2 = hub2.rename(columns={\"CLOSE\": \"hub2_CLOSE\"})\n",
    "    hub1_hub2_diff = pd.DataFrame(hub1[\"hub1_CLOSE\"] - hub2[\"hub2_CLOSE\"], columns=[\"hub1_hub2_diff\"], index=hub1.index)\n",
    "\n",
    "    for i in range(window_size, window_size + 6):\n",
    "        hub1[f\"hub1_CLOSE-{i- window_size}\"] = hub1[\"hub1_CLOSE\"].shift(i)\n",
    "        hub2[f\"hub2_CLOSE-{i - window_size}\"] = hub2[\"hub2_CLOSE\"].shift(i)\n",
    "        hub1_hub2_diff[f\"hub1_hub2_diff-{i - window_size}\"] = hub1_hub2_diff[\"hub1_hub2_diff\"].shift(i)\n",
    "\n",
    "    data = pd.concat([hub1, hub2, hub1_hub2_diff], axis=1).dropna()\n",
    "\n",
    "    features = [\n",
    "        'hub1_CLOSE-0',\n",
    "        'hub2_CLOSE-0',\n",
    "        'hub1_hub2_diff-0', 'hub1_hub2_diff-1', 'hub1_hub2_diff-2',\n",
    "        'hub1_hub2_diff-3', 'hub1_hub2_diff-4', 'hub1_hub2_diff-5',\n",
    "    ]\n",
    "\n",
    "    X = data[features].values\n",
    "    y = data[['hub1_CLOSE', 'hub2_CLOSE']].values\n",
    "\n",
    "    X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "    y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "\n",
    "    X_train, X_val = X_train[:-validation_size], X_train[-validation_size:]\n",
    "    y_train, y_val = y_train[:-validation_size], y_train[-validation_size:]\n",
    "\n",
    "    keras.utils.set_random_seed(42)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2, activation='relu'),\n",
    "        Dense(2)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mape')\n",
    "    model.fit(X_train, y_train, epochs=25, batch_size=1, shuffle=False, verbose=0)\n",
    "\n",
    "    val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "hub1_name = \"the\"\n",
    "hub2_name = \"nbp\"\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 2.0e-3, 3.5e-3)\n",
    "\n",
    "    validation_loss = optuna_ann_system(hub1_name=hub1_name, hub2_name=hub2_name, validation_size=250, test_size=250, window_size=5, lr=lr)\n",
    "    return validation_loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best learning rate found\n",
    "print(\"Best learning rate:\", study.best_params['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1285, 8) (250, 8)\n",
      "Epoch 1/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 10.4395\n",
      "Epoch 2/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549us/step - loss: 7.3005\n",
      "Epoch 3/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 7.2642\n",
      "Epoch 4/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563us/step - loss: 7.2221\n",
      "Epoch 5/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 545us/step - loss: 7.1874\n",
      "Epoch 6/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 573us/step - loss: 7.1625\n",
      "Epoch 7/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643us/step - loss: 7.1675\n",
      "Epoch 8/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565us/step - loss: 7.1987\n",
      "Epoch 9/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567us/step - loss: 7.1700\n",
      "Epoch 10/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565us/step - loss: 7.1903\n",
      "Epoch 11/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567us/step - loss: 7.1698\n",
      "Epoch 12/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571us/step - loss: 7.2013\n",
      "Epoch 13/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 7.1968\n",
      "Epoch 14/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560us/step - loss: 7.1712\n",
      "Epoch 15/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654us/step - loss: 7.2549\n",
      "Epoch 16/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557us/step - loss: 7.2475\n",
      "Epoch 17/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564us/step - loss: 7.2805\n",
      "Epoch 18/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 556us/step - loss: 7.2630\n",
      "Epoch 19/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561us/step - loss: 7.2755\n",
      "Epoch 20/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 7.3228\n",
      "Epoch 21/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550us/step - loss: 7.4170\n",
      "Epoch 22/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 7.4764\n",
      "Epoch 23/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616us/step - loss: 7.5267\n",
      "Epoch 24/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568us/step - loss: 7.4550\n",
      "Epoch 25/25\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565us/step - loss: 7.4760\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6827\n",
      "Test Loss: 5.974734783172607\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MAPE for the: 5.83%\n",
      "MAPE for nbp: 6.12%\n"
     ]
    }
   ],
   "source": [
    "hub1, hub2 = ann_system(hub1_name, hub2_name, 250, 250, 5, mode = \"test\", lr=study.best_params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
